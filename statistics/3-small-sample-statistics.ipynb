{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small sample statistics\n",
    "\n",
    "Szymon Talaga | 30 March 2020\n",
    "\n",
    "![ZIP logo](zip.png)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(c(\"latex2exp\"))\n",
    "# Packages that we will need\n",
    "library(tidyverse)    # read in core tidyverse packages at once\n",
    "library(latex2exp)    # for easy math on plots\n",
    "\n",
    "# Set default theme for ggplot2\n",
    "theme_set(theme_bw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we review and implement in `R` the methods discussed in chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing normality\n",
    "\n",
    "Here we show how we can draw QQ plots and run Kolmogorv-Smirnov and Shapiro-Wilk tests.\n",
    "\n",
    "We start by examining QQ plots for three cases. \n",
    "For quick QQ plotting we will use base implementation, but later on also the GGPlot approach will be discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "###################################\n",
    "### Normal distribution (n = 100)\n",
    "###################################\n",
    "x_normal <- rnorm(100, mean = 100, sd = 15)\n",
    "\n",
    "hist(x_normal, col = \"lightgreen\")\n",
    "qqnorm(x_normal, xlab = \"Normal quantiles\", ylab = \"Observed quantiles\")\n",
    "qqline(x_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, agreement with normal distribution is very good. This is not surprising since we know our data is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(101)\n",
    "### Exponential distribution (n = 100)\n",
    "x_exponential <- rexp(100, rate = 1/10)\n",
    "\n",
    "hist(x_exponential, col = \"lightgreen\")\n",
    "qqnorm(x_exponential, xlab = \"Normal quantules\", ylab = \"Observed quantiles\")\n",
    "qqline(x_exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see an important kind of a pattern on QQ plot, so it is crucial we understand it.\n",
    "Observed quantiles on the left (low values) are higher than expected. This means that the left tail\n",
    "is shrinked in comparison to a normal distribution. This makes sense, since we know that\n",
    "exponential distribution is bounded from below by $0$.\n",
    "\n",
    "On the other hand observed quantiles on the right (high values) are also higher than expected.\n",
    "In this case this means that the right tail is stretched (longer) in comparison to a normal distribution.\n",
    "In other words, high values are much more frequent than in a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(104)\n",
    "##############################################\n",
    "### t distribution with 3 degrees of freedom\n",
    "#############################################\n",
    "# It is more or less like normal, but has much fatter, longer tails.\n",
    "x_t <- rt(100, df = 3)\n",
    "\n",
    "hist(x_t, col = \"lightgreen\")\n",
    "qqnorm(x_t, xlab = \"Normal quantiles\", ylab = \"Observed quantiles\")\n",
    "qqline(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the left tail (low values) has lower quantiles than expected\n",
    "which means that it is longer than in the corresponding normal distribution\n",
    "with the same mean and variance.\n",
    "\n",
    "The right tail has higher quantiles than expected which also means\n",
    "that it is longer than in the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(10101)\n",
    "##################################\n",
    "### Uniform in the range -3 to 3\n",
    "##################################\n",
    "x_uniform <- runif(100, -3, 3)\n",
    "\n",
    "hist(x_uniform, col = \"lightgreen\")\n",
    "qqnorm(x_uniform, xlab = \"Normal quantiles\", ylab = \"Observed quantiles\")\n",
    "qqline(x_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a pattern characteristic for a distribution which is truncated.\n",
    "Note how observed quantiles flattens out at $-3$ and $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will draw a more complex plot with histograms and QQ plots for three different distributions\n",
    "using GGPlot. Then we will use this data to run Kolmogorov-Smirnov and Shapiro-Wilk tests and learn how they work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number of data points\n",
    "N <- 100\n",
    "### Dataset\n",
    "### `tibble` is enhanced data.frame class provided by the tidyverse suite of packages.\n",
    "### It works like stadard `data.frame` but prints in a nicer way and have some other useful features.\n",
    "df <- tibble(\n",
    "    normal = rnorm(N, mean = 0, sd = 1),\n",
    "    t_df3  = rt(N, df = 3),\n",
    "    t_df10 = rt(N, df = 10),\n",
    "    t_df20 = rt(N, df = 20)\n",
    ")\n",
    "\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw the plot easily we will reshape our data and cast it to the long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long <- df %>%\n",
    "    pivot_longer(c(normal, t_df3, t_df10, t_df20))\n",
    "\n",
    "head(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HISTOGRAMS\n",
    "df_long %>%\n",
    "    ggplot(aes(x = value, fill = name)) +\n",
    "    geom_histogram(color = \"black\") +\n",
    "    facet_wrap(~name, ncol = 2, nrow = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long %>%\n",
    "    ggplot(aes(sample = value, color = name)) +\n",
    "    geom_qq_line(color = \"black\") +\n",
    "    geom_qq() +\n",
    "    facet_wrap(~name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we see significant deviations only in the case of the $t$ distribution with $3$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KS test\n",
    "## The second argument is the function\n",
    "## implementing CDF of normal distribution\n",
    "\n",
    "## KS test for t distribution with 3 degrees of freedom\n",
    "ks.test(df$t_df3, pnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SW test for t distribution with 3 degrees of freedom\n",
    "shapiro.test(df$t_df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, Shapiro-Wilk test is clearly much more powerful than Kolmogorov-Smirnov test\n",
    "since it yielded significant results (for $t$ with $df = 3$ which is not normal) and KS test did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro.test(df$t_df10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see what happens in larger samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number of data points\n",
    "N <- 2000\n",
    "### Dataset\n",
    "### `tibble` is enhanced data.frame class provided by the tidyverse suite of packages.\n",
    "### It works like stadard `data.frame` but prints in a nicer way and have some other useful features.\n",
    "df <- tibble(\n",
    "    normal = rnorm(N, mean = 0, sd = 1),\n",
    "    t_df3  = rt(N, df = 3),\n",
    "    t_df10 = rt(N, df = 10),\n",
    "    t_df20 = rt(N, df = 20)\n",
    ")\n",
    "\n",
    "df_long <- pivot_longer(df, c(normal, t_df3, t_df10, t_df20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long %>%\n",
    "    ggplot(aes(sample = value, color = name)) +\n",
    "    geom_qq_line(color = \"black\") +\n",
    "    geom_qq() +\n",
    "    facet_wrap(~name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro.test(df$t_df20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we see that even for $t$ with $df = 20$ we get significant result\n",
    "indicating lack of normality. Without looking at the QQ plot it could be tempting\n",
    "to decide that the sample is not normal and choose for instance a non-parametric rank test\n",
    "while the deviation is small enough that it is quite okay to use a $t$ test.\n",
    "\n",
    "We show this with a simple numerical simulation in which we estimate the type I error\n",
    "when running $t$ tests with data generated from a $t$ distribution with $20$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE ################\n",
    "## THIS MAY TAKE A WHILE\n",
    "########################\n",
    "R <- 100000L\n",
    "\n",
    "pvals <- vector(mode = \"numeric\", length = R)\n",
    "for (i in 1:R) {\n",
    "    data <- rt(nrow(df), df = 20)\n",
    "    pval <- t.test(data)$p.value\n",
    "    pvals[i] <- pval\n",
    "}\n",
    "\n",
    "mean(pvals <= .05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal sample variance\n",
    "\n",
    "In this section we show the basic methods for statistical inference concerning sample variances.\n",
    "We focus on the case of normal variances, but in the end we will show also a more general method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single normal sample variance ($\\chi^2$ distribution)\n",
    "\n",
    "We will compute a $p$-value for hypothesis\n",
    "$$\n",
    "H_0: \\sigma^2 = 10\n",
    "$$\n",
    "and construct a confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(333)\n",
    "## DATA\n",
    "x <- rnorm(100, mean = 12, sd = sqrt(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2_0 <- 10\n",
    "s2 <- var(x)\n",
    "df <- length(x) - 1\n",
    "\n",
    "chisq_statistic <- df * s2 / sigma2_0\n",
    "chisq_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue <- 1 - pchisq(chisq_statistic, df = df)\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci95_twosided <- df*s2 / c(qchisq(.975, df = df), qchisq(.025, df = df))\n",
    "ci95_twosided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of two normal sample variances ($F$ distribution)\n",
    "\n",
    "Now we compare variances from two normal samples. \n",
    "First we do it by hand and then compare our results with\n",
    "`var.test` function implemented in base `R`.\n",
    "\n",
    "The null hypothesis is:\n",
    "$$\n",
    "H_0: \\frac{\\sigma^2_1}{\\sigma^2_2} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 <- rnorm(100, mean = 100, sd = 15)\n",
    "x2 <- rnorm(100, mean = 100, sd = 18)\n",
    "\n",
    "null_ratio <- 1\n",
    "\n",
    "s2_1 <- var(x1)\n",
    "df1  <- length(x1) - 1\n",
    "\n",
    "s2_2 <- var(x2)\n",
    "df2  <- length(x2) - 1\n",
    "\n",
    "F_statistic <- (s2_1 / s2_2) / null_ratio\n",
    "F_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TWO-SIDED P-VALUE\n",
    "pvalue <-  2 * pf(F_statistic, df1 = df1, df2 = df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TWO-SIDED CONFIDENCE INTERVAL\n",
    "ci95 <- F_statistic / c(qf(0.975, df1 = df1, df2 = df2), qf(0.025, df1 = df1, df2 = df2))\n",
    "ci95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var.test(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $t$ test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to conduct $t$ tests in `R`.\n",
    "We will save ourselves the trouble of doing this by hand,\n",
    "since we already implemented a lot of $z$ tests this way\n",
    "and the procedure is exactly the same, only the distribution\n",
    "of the test statistic we use is different.\n",
    "\n",
    "Instead, we will focus on `R` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One sample $t$ test\n",
    "\n",
    "We will use the following hypotheses:\n",
    "$$\n",
    "H_0: \\mu \\leq 9\n",
    "$$\n",
    "$$\n",
    "H_1: \\mu > 9\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- rnorm(20, mean = 10, sd = 3)\n",
    "\n",
    "t.test(x, mu = 9, alternative = \"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two samples (paired)\n",
    "\n",
    "We will test the null:\n",
    "$$\n",
    "H_0: \\mu_1 = \\mu_2\n",
    "$$\n",
    "And use $\\alpha = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 <- rnorm(15, mean = 10, sd = 3)\n",
    "x2 <- rnorm(15, mean = 12, sd = 3)\n",
    "\n",
    "## SIGNIFICANCE LEVEL\n",
    "alpha <- 0.01\n",
    "\n",
    "##DIFFERENCES\n",
    "diff <- x2 - x1\n",
    "\n",
    "t.test(diff, conf.level = 1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALTERNATIVE APPROACH\n",
    "t.test(x2, x1, paired = TRUE, conf.level = 1 - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two samples (independent)\n",
    "\n",
    "We will test the null:\n",
    "$$\n",
    "H_0: \\mu_1 = \\mu_2\n",
    "$$\n",
    "And use $\\alpha = 0.05$ (default value in `R`).\n",
    "\n",
    "We will use simulated samples with different variances\n",
    "and compare results with and without the Welsch-Satterthwaite\n",
    "correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(44)\n",
    "\n",
    "x1 <- rnorm(15, mean = 100, sd = 15)\n",
    "x2 <- rnorm(15, mean = 107, sd = 25)\n",
    "\n",
    "t.test(x1, x2, var.equal = TRUE) ## by default var.equal = FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.test(x1, x2, var.equal = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar, but note that the $p$-value without the correction is lower.\n",
    "This is due to the fact that corrected degrees of freedom are lower.\n",
    "Therefore, without the correction we inflate the type I error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $t$ test as $z$ test\n",
    "\n",
    "Here we will convince ourselves that $t$ test functions can be used in lieu of $z$ test functions\n",
    "in large samples. This is useful, because $z$ tests are usually not implemented in statistical packages.\n",
    "\n",
    "We will study the distribution of sample mean of variables that are ''normalish'', but not exactly.\n",
    "In particular, they will be slightly right-skewed and bounded from below by$0$. \n",
    "A very typical scenario in real world data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1010)\n",
    "\n",
    "x1 <- rgamma(100, shape = 3, rate = 4)\n",
    "x2 <- rgamma(100, shape = 4, rate = 5)\n",
    "\n",
    "hist(x1, col = \"lightgreen\")\n",
    "hist(x2, col = \"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## T TEST RESULT\n",
    "t.test(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Z TEST RESULT\n",
    "dbar <- mean(x1) - mean(x2)\n",
    "se   <- sqrt(var(x1) / length(x1) + var(x2) / length(x2))\n",
    "z    <- dbar / se\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue <- 2 * (1 - pnorm(abs(z)))\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p$-values we get differ only on the $4$th decimal digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci95 <- dbar + c(-1, 1) * se * qnorm(.975)\n",
    "ci95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence interval bounds also differ only on $4$th decimal digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a $t$ test with the correction for unequal variances. Usually in large samples it will not make any\n",
    "visible difference. However, if corrected degrees of freedom are markedly reduced this may suggest\n",
    "that the sample distributions may be very unbalanced with respect to variance and perhaps also sample sizes\n",
    "and this may slow down the rate of convergence of CLT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $t$ test power analysis\n",
    "\n",
    "Power calculations are always hard and usually analytically intractable.\n",
    "That is why it is nice that base `R` provides us with a function for examining\n",
    "power of $t$ tests.\n",
    "\n",
    "We can use it in two different ways. Either to calculate minimum sample size\n",
    "needed to gaurantee a given level of power conditioned on specific $\\alpha$\n",
    "and minimum effect of interest or to calculate power of a test with specific\n",
    "alpha, minimum effect and sample size.\n",
    "\n",
    "We will study the case of a difference between two samples\n",
    "with two-sided $\\alpha = 0.05$ and minimum detectable difference of $1$\n",
    "with expected standard deviation of $2$. We will want to have power of $1 - \\beta = 0.90$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATING MINIMUM REQUIRED SAMPLE SIZE\n",
    "alpha <- 0.05    # significance level\n",
    "delta <- 1\n",
    "stdev <- 2\n",
    "power <- 0.90\n",
    "\n",
    "power.t.test(delta = delta, sd = stdev, sig.level = alpha, power = power, strict = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will consider a case in which we already collected data but want to estimate the power of our test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n     <- 30\n",
    "alpha <- 0.05\n",
    "delta <- 1\n",
    "stdev <- 2\n",
    "\n",
    "power.t.test(n = n, delta = delta, sd = stdev, sig.level = alpha, strict = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how small is the power. This is an important point. Small sample sizes usually provides us with little\n",
    "statistical power. This is why it is so important to remember that not rejecting $H_0$ does not necessarily mean\n",
    "it is true. For the same reason, this is why it is almost always good to do power analysis when planning an experiment.\n",
    "This way you may determine how many subjects do you need in order to give you a fighting chance for detecting an\n",
    "effect you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 | Growth rate analysis\n",
    "\n",
    "Here we will try to do a simple analysis of data about COVID-19 cases. For simplicity we will limit our attention to cases in Poland.\n",
    "We will focus on estimating the growth rate of the process, since this is one of the most important property of epidemic processes\n",
    "such as the ongoing teribble pandemic.\n",
    "\n",
    "Our analysis will be very simple, so we should not be to attached to the results we will produce. In particular, in this first approach\n",
    "we will assume that the growth rate is stable in time, which is most probably untrue. However, we do not have proper tool to deal\n",
    "with a trend yet, so we have to resort to such a simplifying assumption.\n",
    "\n",
    "We will use a [dataset](https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv) with time series of COVID-19 cases by countries/regions compiled by John Hopkins University.\n",
    "\n",
    "This analysis is based on the version of the file from 28.03.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read_csv(\"../data/time_series_covid19_confirmed_global.csv\")\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET DATA FOR POLAND\n",
    "## DROP UNNESECCARY COLUMNS\n",
    "## AND CAST TO LONG FORMAT\n",
    "## AND LIMIT DATA TO DAYS WITH NON-ZERO NUMBER OF CASES\n",
    "PL <- data %>%\n",
    "    filter(`Country/Region` == \"Poland\") %>%\n",
    "    select(-`Province/State`, -`Country/Region`, -Lat, -Long) %>%\n",
    "    pivot_longer(everything(), names_to = \"date\", values_to = \"cases\") %>%\n",
    "    filter(cases > 0) %>%\n",
    "    mutate(date = fct_inorder(factor(date)))\n",
    "\n",
    "PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK DISTRIBUTION\n",
    "PL %>%\n",
    "    ggplot(aes(x = date, y = cases)) +\n",
    "    geom_bar(color = \"black\", fill = \"lightgreen\", stat = \"identity\") +\n",
    "    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n",
    "    xlab(\"\") +\n",
    "    ylab(\"Number of COVID-19 cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is clearly not normal and grows quasi-exponentially. But this is not a problem (in terms of statistics), since we are\n",
    "not interested in raw numbers but rather in rate of growth. So we should rather look at ratios:\n",
    "$$\n",
    "\\frac{x_{t+1}}{x_{t}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPUTE RATIOS\n",
    "## WE LOSE THE FIRST DATA POINT FOR WHICH WE CAN NOT COMPUTE RATIO\n",
    "PL$ratio <- PL$cases / lag(PL$cases, k = 1)\n",
    "\n",
    "## CHECK DISTRIBUTION\n",
    "PL %>%\n",
    "    ggplot(aes(x = date, y = ratio)) +\n",
    "    geom_bar(color = \"black\", fill = \"lightgreen\", stat = \"identity\") +\n",
    "    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n",
    "    xlab(\"\") +\n",
    "    ylab(\"Growth rate of COVID-19 cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for some spikes in the very beginning when there were just few cases the rate of growth seems to be relatively stable.\n",
    "Thus, it is best to drop data for first five days as data for early day is likely to have been driven by a different data generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL2 <- slice(PL, 6:nrow(PL))\n",
    "\n",
    "## CHECK DISTRIBUTION\n",
    "PL2 %>%\n",
    "    ggplot(aes(x = date, y = ratio)) +\n",
    "    geom_bar(color = \"black\", fill = \"lightgreen\", stat = \"identity\") +\n",
    "    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n",
    "    xlab(\"\") +\n",
    "    ylab(\"Growth rate of COVID-19 cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a time series data, so before we can proceed further we should check whether we really assume that observations are independent.\n",
    "To do this we can construct a plot of so-called autocorrelation function. It shows how much correlated are observations at time\n",
    "$t$ with observations at time $t - k$ where $k$ is the number of time steps of difference usually called _lag_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf(filter(PL2, !is.na(ratio))$ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue bands show the acceptable level of observed correlations under the assumption of now correlation in the data generating process.\n",
    "So it seems we can safely assume that our observations of ratios are IID.\n",
    "\n",
    "On the other hand the data does not look very ''normal'' and the sample is relatively small, so perhaps the best thing\n",
    "we can do (although still rather inaccurate) is to use bootstrap.\n",
    "\n",
    "More specifically, since we are interested in the growth rate we will estimate its expected value but also standard deviation to check\n",
    "how volatile is the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD `boot` LIBRARY FOR BOOTSTRAP ROUTINES\n",
    "library(boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(303)\n",
    "## FUNCTION COMPUTING MEAN AND STANDARD DEVIATION\n",
    "statistic <- function(data, indexes) {\n",
    "    boot_data <- data[indexes]\n",
    "    c(\n",
    "        xbar = mean(boot_data, na.rm = TRUE),\n",
    "        s2   = var(boot_data, na.rm = TRUE)\n",
    "    )\n",
    "}\n",
    "\n",
    "## BOOTSTRAP RESULT (10000 replicates)\n",
    "result <- boot(PL2$ratio, statistic, R = 10000)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boostrap distribution of average growth looks quite normal.\n",
    "\n",
    "The expected growth rate is about $29.9\\%$ a day.\n",
    "\n",
    "Remember that in this analysis we assumed that the true growth rate of the data generating process is stable.\n",
    "This is rather untrue, but given the distribution of growth ratios that we plotted it can be considered a rough approximation\n",
    "to what is happening right now. Nonetheless, we should be rather cautious when interpreting our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_boot <- result$t[, 1]\n",
    "summary(growth_boot)\n",
    "\n",
    "hist(growth_boot, col = \"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 95% PERCENTILE INTERVAL\n",
    "print(quantile(growth_boot, probs = c(0.025, 0.975)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our modest estimation suggests that true growth rate of the process may be somewhere in between $23.3\\%$ and $36.7\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use our results to estimate the expected number of days in which the numebr of cases doubles.\n",
    "To do this we have to solve the following equation:\n",
    "$$\n",
    "g^k = 2\n",
    "$$\n",
    "where $g$ is a growth rate.\n",
    "\n",
    "To solve it we start by taking a natural logarithm of both sides, which yields:\n",
    "$$\n",
    "k\\log{g} = \\log{2}\n",
    "$$\n",
    "So:\n",
    "$$\n",
    "k = \\frac{\\log{2}}{\\log{g}}\n",
    "$$\n",
    "\n",
    "Thus, using our the expected growth rate we get:\n",
    "$$\n",
    "k \\approx 2.65\\text{ days}\n",
    "$$\n",
    "\n",
    "The lower bound based on the $95\\%$ two-sided percentile bootstrap confidence interval is:\n",
    "$$\n",
    "k_{\\text{low}} \\approx 3.31\\text{ days}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both estimates are markedly lower from estimated growth rates based on slightly more advanced models such as those given\n",
    "on an excellent blog [Fizyk Wyjaśnia](http://www.fizykwyjasnia.pl/na-biezaco/prognozy-rozwoju-epidemii-koronawirusa/)\n",
    "(unfortunatelly it is only in Polish).\n",
    "\n",
    "Next time, once we learn some more statistics, we will try to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason our analysis is not very accurate is the fact that we did not take into account a decreasing trend of growth rates in time.\n",
    "We can see it on a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL %>%\n",
    "    ggplot(aes(x = seq_along(ratio)-1, y = ratio)) +\n",
    "    geom_point(size = 4) +\n",
    "    geom_smooth(method = \"loess\", se = FALSE) +\n",
    "    xlab(\"Days after the first case\") +\n",
    "    ylab(\"Growth rate of COVID-19 cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try to do a similar analysis for some other countries. Remember to consider how likely is the assumption of growth rate stability in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
